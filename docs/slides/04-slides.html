<!DOCTYPE html>
<html>
  <head>
    <title>Day 4</title>
    <meta charset="utf-8">
    <meta name="author" content="Michael W. KearneyðŸ“Š School of Journalism  Informatics Institute  University of Missouri" />
    <link href="lib/remark-css/robot.css" rel="stylesheet" />
    <link href="lib/remark-css/robot-fonts.css" rel="stylesheet" />
    <link href="lib/font-awesome/css/all.css" rel="stylesheet" />
    <link href="lib/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Day 4
## Correlations and factor analysis
### Michael W. KearneyðŸ“Š<br/>School of Journalism <br/>Informatics Institute <br/>University of Missouri
### <table style="border-style:none;padding-top:30px;" class=".table">
<tr>
<th style="padding-right:75px!important">
<a href="https://twitter.com/kearneymw"> <i class="fa fa-twitter"></i> </a>
</th>
<th style="padding-left:75px!important">
<a href="https://github.com/mkearney"> <i class="fa fa-github"></i> </a>
</th>
</tr>
<tr style="background-color:#fff">
<th style="padding-right:75px!important">
<a href="https://twitter.com/kearneymw"> <span class="citation">@kearneymw</span> </a>
</th>
<th style="padding-left:75px!important">
<a href="https://github.com/mkearney"> <span class="citation">@mkearney</span> </a>
</th>
</tr>
</table>

---

class: inverse, center, middle



## Agenda

---

## Agenda
1. Co-variance
   - What is covariance?
   - Calculating covariance
1. Correlations
   - Bivariate correlations
   - Correlation matrices
1. Factor analysis
   - What is a factor?
   - Calculating scores
   - Reliability

---
class: inverse, center, middle

## Test

Try this out:


```r
source("https://raw.githubusercontent.com/mkearney/stat/master/static/build.R")
```

---
class: inverse, center, middle

## Co-variance

---

## Variation (review)

**Variance**: average distance from the mean

`$$s^2=\frac{\sum (x - \bar{x})^2}{n - 1}$$`

+ Single variable: `\(x\)`
+ Always positive: `\((x - \bar{x})^2\)`


---

## Co-variation

**Covariance**: average *related* distance**s** from the mean

`$$cov_{(x, y)} = \frac{\sum (x-\bar{x})(y-\bar{y})}{n - 1}$$`

+ Two variables: `\(x\)` and `\(y\)`
+ Can be positive or negative
   - Positive means vary [above/below mean] together
   - Negative means vary [above/below mean] inversely

---

## Note:

When a variable covaries with itself...

`$$cov_{(x, x)} = \frac{\sum (x-\bar{x})(x-\bar{x})}{n - 1}$$`

`$$cov_{(x, x)} = \frac{\sum (x - \bar{x})^2}{n - 1}$$`

`$$cov_{(x, x)} = s^2$$`

&gt; Covariance with self is the same as variance

---

## Correlation

A **correlation** describes how variation in one variable relates to variation in another variable. 

In statistics, correlations describe the **magnitude** and **direction** of a relationship between two variables. 

---

## Example code...


```r
## create data frame with x and y coming from random normal draws
df &lt;- data_frame(x = rnorm(20), y = x + rnorm(20))

## plot points
ggplot(df, aes(x, y)) + 
  geom_vline(xintercept = 0, color = "#666666") + 
  geom_hline(yintercept = 0, color = "#666666") + 
  geom_point(shape = 21, fill = "#F1B82D", size = 8, alpha = .8) + 
  labs(title = "Scatter plot of y by x", 
    x = "Observations of x", y = "Observations of y") + 
  ggsave("img/scatter_plot.png", width = 8, height = 7, units = "in")
```

---

&lt;p style="align:center"&gt; &lt;img src="img/scatter_plot.png" /&gt; &lt;/p&gt;

---

## Correlation coefficient

**`r`**: describes the magnitude (size) and direction (order) of a relationship

There are several types of correlation coefficients, but typically they all...

+ have a **range of possible values** from -1.0 to +1.0
+ **describe** the magnitude (size) and direction (order) of an association
   - **Direction** by **sign** (positive or negative)
   - **Magnitude** by **absolute value** (distance from zero)

---

## Note(s) on "coefficients"

&gt; Throughout statistics, a *coefficient* refers to the multiplicative factor (a numeric estimate) for one variable in relation to another variable. 

&gt; In **regression**, coefficients are often referred to as "beta coefficients" or "beta weights"

&gt; In **machine learning**, coefficients are often referred to as [simply] "weights"

---

## Note(s) on notation

In many statistical modeling frameworks, a correlation is often indicated (by convention) by connecting two variables [names] with a double tilde `~~` symbol.

I'll occasionally use this kind of syntax to mean **correlation**

```
x ~~ y
```

---


## Types of correlations

**Pearson product-moment**: linear relationship between two variables 
`$$r_{(x,y)} = x\sim\sim y$$`

**Spearman's rho**: relationship between rankings of two variables 
`$$\rho_{(x,y)} = x\sim\sim y$$`

**Intraclass**: relationship between paired observations 
`$$\varrho_{(x_{t1},x_{t2})} = x_{t1}\sim\sim x_{t2}$$`

---

## Pearson's `r`

**Correlation**: the covariation divided by the total variation

`$$r_{(x, y)} = \frac{cov_{(x, y)}}{s_{x}s_{y}}$$`

+ `\(s_{x}\)` and `\(s_{y}\)` are the sample standard deviations of `\(x\)` and `\(y\)` 
   - *Note*: `\(s\)` is the square root of the variance, i.e., `\(\sqrt{s^2}\)`

---

## Note(s) on `x ~~ x`

A single variable `\(x\)` correlates perfectly with itself...

`$$r_{(x, x)} = \frac{cov_{(x, x)}}{s_{x}s_{x}}$$`

+ We know from earlier `\(cov_{(x,x)}\)` is equal to `\(s_{x}^2\)` (variance)
+ And `\(s_{x}s_{x}\)` can be rewritten as `\(s_{x}^2\)`, so...

`$$r_{(x, x)} = \frac{s_{x}^2}{s_{x}^2} = 1.0$$`


---

## Numeric and rank-order data


```r
## create data frame with x and y coming from random normal draws
df &lt;- data_frame(x = rnorm(20), y = x + rnorm(20))

## convert to ranks
dfo &lt;- map_df(df, factor) %&gt;% mutate_all(as.numeric)

## print
print(df, n = 6)
```

|   x    |   y    |
|:------:|:------:|
| 0.139  | 1.290  |
| -1.517 | 0.075  |
| 1.827  | 3.271  |
| 1.549  | 0.934  |
| 1.383  | 0.842  |
| -0.808 | -0.953 |

---

## `cor(method = "pearson")`

By default, `cor()` returns the Pearson product-moment correlation.


```r
## using cor defaults
cor(df$x, df$y)
#&gt; [1] 0.670893

## same thing as
cor(df$x, df$y, method = "pearson")
#&gt; [1] 0.670893
```

Use `method` to specify the type of correlation and `use` to deal with missing data


```r
## pearson product-moment correlation
cor(df$x, df$y, method = "pearson", use = "pairwise.complete.obs")
#&gt; [1] 0.670893
```

---

## `cor(method = ` "spearman"`)`

Get Spearman Rho rank-order correlation.


```r
## spearman rho correlation of df
cor(df$x, df$y, method = "spearman")
#&gt; [1] 0.568421
```

This should be equal to Pearson correlation of the rank order data set


```r
## spearman rho correlation
cor(dfo$x, dfo$y, method = "pearson")
#&gt; [1] 0.568421
```



---

## Rank order correlation

Calculate **Spearman's rho** the same way, only convert observations to ranked values first.

**Example**: converting observations to ranked values

+ `c(22, 2, 5, 1)` converts to `c(4, 2, 3, 1)`
+ `c(7.2, 3.0, 5.25, 1.9)` converts to `c(4, 2, 3, 1)`



---

## Basic example

Let's say we have four values of x and y, and we want to estimate Pearson's R


```r
x &lt;- c(22, 6, 15, 1)
y &lt;- c(1, 3, 9, 10)
cor(x, y, method = "pearson")
#&gt; [1] -0.548093
```

Now estimate Spearman's rho using the same values


```r
cor(x, y, method = "spearman")
#&gt; [1] -0.8
```

Notice a difference?

---

## Example

We can actually replicate the result from the previous slide using Pearson's method if we first convert x and y to ranks


```r
x_ranks &lt;- c(4, 2, 3, 1)
y_ranks &lt;- c(1, 2, 3, 4)
cor(x_ranks, y_ranks, method = "pearson")
#&gt; [1] -0.8
```

Create data frame to visualize difference...


```r
df &lt;- data_frame(
  data = c(rep("raw", 4), rep("rank", 4)), 
  x = c(x, x_ranks), 
  y = c(y, y_ranks))
```

---



```r
ggplot(df, aes(x = x, y = y, fill = data)) + 
  geom_point(shape = 21, size = 8, alpha = .8) + 
  labs(title = "Scatter plot of raw and rank-order values") + 
  scale_fill_manual(values = c(raw = "#F1B82D", rank = "#222222")) + 
  ggsave("img/scatter_plot-xy.png", width = 8, height = 4.5, units = "in")
```

&lt;p style="align:center"&gt; &lt;img src="img/scatter_plot-xy.png" /&gt; &lt;/p&gt;

---

## Hypothesis testing

To conduct a signficiance test of a correlation using R, use `cor.test()`. 


```r
## correlation test
cor.test(df$x, df$y, method = "pearson", alternative = "two.sided")
#&gt; 
#&gt; 	Pearson's product-moment correlation
#&gt; 
#&gt; data:  df$x and df$y
#&gt; t = -0.2204, df = 6, p-value = 0.833
#&gt; alternative hypothesis: true correlation is not equal to 0
#&gt; 95 percent confidence interval:
#&gt;  -0.747114  0.656509
#&gt; sample estimates:
#&gt;        cor 
#&gt; -0.0896272
```

.footnote[To convert the output to a more usable data frame, use `tidy()` from the **{broom}** package.]

---

## `method =` "pearson"

Correlation test of interval/ratio data

```r
## correlation sig test
cor.test(df$x, df$y, method = "pearson") %&gt;%
  broom::tidy() %&gt;% print()
```

| estimate | statistic | p.value | parameter | conf.low | conf.high |                method                | alternative |
|:--------:|:---------:|:-------:|:---------:|:--------:|:---------:|:------------------------------------:|:-----------:|
|  -0.09   |   -0.22   |  0.833  |     6     |  -0.747  |   0.657   | Pearson's product-moment correlation |  two.sided  |

---

## `method =` "spearman"
 
Correlation test of ordinal data

```r
## correlation sig test
cor.test(df$x, df$y, method = "spearman") %&gt;%
  broom::tidy() %&gt;% print()
#&gt; Warning in cor.test.default(df$x, df$y, method = "spearman"): Cannot
#&gt; compute exact p-value with ties
```

| estimate | statistic | p.value |             method              | alternative |
|:--------:|:---------:|:-------:|:-------------------------------:|:-----------:|
|  -0.448  |  121.673  |  0.265  | Spearman's rank correlation rho |  two.sided  |


---

## Correlation tool

Visualize correlations of different values (-1 to 1):

+ https://mikewk.shinyapps.io/correlation/

---

## Practice

Guess the correlation coefficient (the game):

+ http://guessthecorrelation.com/




---
class: inverse, center, middle

## Factors and factor analysis

---

## Factor

A **factor** is a psychometric term for variable. 

In statistics, factors describe the **latent** variables which we are attempting to measure. 


---

## Items

An **item** refers to a single question/prompt/response-provoking stimulus in a questionnaire.

In the context of a study, a **variable** refers to a construct (or factor) to be examined.

Variables can consist of one or more items from a questionnaire.


---

## Multi-item variables

When a variable (or factor) is measured using multiple items, we still want to represent it with one number.

**How can we represent a variable measured with 5 likert-type items using one number per observation (respondent)?**


---

## Example

Our variable of interest is extraversion/introversion. We measure it using four likert-type items:

+ I like talking to lots of people
+ I like attending events where I meet knew people
+ I am comfortable at a party where I don't know anyone else
+ Meeting my friends' friends makes me nervous


---

## Example responses


```r
## first person's responses
person1 &lt;- c(1, 2, 6, 7)
person2 &lt;- c(1, 1, 6, 7)
person3 &lt;- c(2, 1, 6, 6)
person4 &lt;- c(2, 1, 7, 7)
person5 &lt;- c(2, 1, 6, 6)

## convert to data frame with four columns
peeps &lt;- c(person1, person2, person3, person4, person5) %&gt;% 
  matrix(nrow = 5, byrow = TRUE) %&gt;%
  as.data.frame()

## view data
print(peeps)
#&gt; | V1 | V2 | V3 | V4 |
#&gt; |:--:|:--:|:--:|:--:|
#&gt; | 1  | 2  | 6  | 7  |
#&gt; | 1  | 1  | 6  | 7  |
#&gt; | 2  | 1  | 6  | 6  |
#&gt; | 2  | 1  | 7  | 7  |
#&gt; | 2  | 1  | 6  | 6  |
```

---

## Correlation matrix

View multiple bivariate correlations in a matrix.


```r
## view correlation matrix
cor(peeps, method = "pearson")
#&gt;           V1        V2        V3        V4
#&gt; V1  1.000000 -0.612372  0.408248 -0.666667
#&gt; V2 -0.612372  1.000000 -0.250000  0.408248
#&gt; V3  0.408248 -0.250000  1.000000  0.408248
#&gt; V4 -0.666667  0.408248  0.408248  1.000000
```


---

## Reliability

Cronbach's alpha is typically used to summarize correlations between the measurement items using a single estimate


```r
suppressWarnings(psych::alpha(peeps, keys = c("V1", "V2"), warnings = FALSE))
#&gt; In factor.stats, I could not find the RMSEA upper bound . Sorry about that
#&gt; 
#&gt; Reliability analysis   
#&gt; Call: psych::alpha(x = peeps, keys = c("V1", "V2"), warnings = FALSE)
#&gt; 
#&gt;   raw_alpha std.alpha G6(smc) average_r    S/N  ase mean   sd median_r
#&gt;          0    -0.073    0.84    -0.017 -0.068 0.71  6.5 0.25   -0.079
#&gt; 
#&gt;  lower alpha upper     95% confidence boundaries
#&gt; -1.38 0 1.38 
#&gt; 
#&gt;  Reliability if an item is dropped:
#&gt;     raw_alpha std.alpha G6(smc) average_r   S/N alpha se var.r med.r
#&gt; V1-      0.19      0.21    0.48     0.083  0.27     0.64  0.19  0.25
#&gt; V2-      0.50      0.46    1.00     0.222  0.86     0.38  0.31  0.41
#&gt; V3      -0.21     -0.46    0.30    -0.118 -0.32     0.81  0.47 -0.41
#&gt; V4      -2.00     -1.58   -0.38    -0.257 -0.61     2.38  0.20 -0.41
#&gt; 
#&gt;  Item statistics 
#&gt;     n raw.r std.r r.cor r.drop mean   sd
#&gt; V1- 5  0.46  0.33  0.36  -0.10  6.4 0.55
#&gt; V2- 5  0.00  0.12 -0.22  -0.41  6.8 0.45
#&gt; V3  5  0.56  0.64  0.70   0.13  6.2 0.45
#&gt; V4  5  0.91  0.86  0.94   0.67  6.6 0.55
#&gt; 
#&gt; Non missing response frequency for each item
#&gt;      1   2   6   7 miss
#&gt; V1 0.4 0.6 0.0 0.0    0
#&gt; V2 0.8 0.2 0.0 0.0    0
#&gt; V3 0.0 0.0 0.8 0.2    0
#&gt; V4 0.0 0.0 0.4 0.6    0
```


---

## Factor analysis

To make sure we have **reliable** measures, we use factor analysis.

**Factor analysis** essentially finds the correlation between responses for similar items. 

There are lots of details and variations, but knowing this much will help you in the future!
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
