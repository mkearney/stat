---
title: "Day 4"
subtitle: "Correlations and factor analysis"
author: '`r rmdees::title_author()`'
date: '`r rmdees::title_icons()`'
output:
  xaringan::moon_reader:
    lib_dir: lib
    css: ["robot", "robot-fonts", "css/slides.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, center, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, collapse = TRUE, comment = "#>")
options(htmltools.dir.version = FALSE)
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
library(tidyverse)
theme_set(tfse::theme_mwk(base_size = 18))
p <- function(x, n = 5) {
  DT::datatable(head(x, n), style = "bootstrap", autoHideNavigation = TRUE,
  fillContainer = FALSE, options = list(pageLength = n))
}
```

## Agenda

---

## Agenda
1. Co-variance
   - What is covariance?
   - Calculating covariance
1. Correlations
   - Bivariate correlations
   - Correlation matrices
1. Factor analysis
   - What is a factor?
   - Calculating scores
   - Reliability
---
class: inverse, center, middle

## Co-variance

---

## Variation (review)

**Variance**: average distance from the mean

$$s^2=\frac{\sum (x - \bar{x})^2}{n - 1}$$

+ Single variable: $x$
+ Always positive: $(x - \bar{x})^2$


---

## Co-variation

**Covariance**: average *related* distance**s** from the mean

$$cov_{(x, y)} = \frac{\sum (x-\bar{x})(y-\bar{y})}{n - 1}$$

+ Two variables: $x$ and $y$
+ Can be positive or negative
   - Positive means vary [above/below mean] together
   - Negative means vary [above/below mean] inversely

---

## Notable:

When a variable covaries with itself...

$$cov_{(x, x)} = \frac{\sum (x-\bar{x})(x-\bar{x})}{n - 1}$$

it's actually just representing the variance:

$$s^2=\frac{\sum (x - \bar{x})^2}{n - 1}$$

---

## Correlation

A **correlation** describes how variation in one variable relates to variation in another variable. 

In statistics, correlations describe the **magnitude** and **direction** of a relationship between two variables. 


---

```{r, fig.show = 'hide', fig.keep = 'none'}
## create data frame with x and y coming from random normal draws
df <- data_frame(x = rnorm(20), y = x + rnorm(20))

## scatter plot
ggplot(df, aes(x, y)) + 
  geom_point() + 
  labs(title = "Scatter plot of y by x") + 
  ggsave("img/scatter_plot.png", width = 8, height = 4.75, units = "in")
```

<p style="align:center"> <img src="img/scatter_plot.png" /> </p>

---

## Correlation coefficient: `r`

**`r`** (correlation coefficient): a statistic that describes the magnitude (size) and direction (order) of a relationship

There are several types of correlation coefficients, but all assume following:

+ The **range of possible values** goes from -1 to +1
+ **Direction** is described by the sign of the coefficient (negative = negative relationship and vice versa)
+ **Magnitude** is described by the coefficient's distance from zero
    + A correlation of zero indicates an absence of correlation (no relationship)

.footnote[In statistics, a **coefficient** refers to the multiplicative factor (a numeric estimate) for one variable in relation to another variable.]

---

## Types of correlations

**Pearson product-moment**: linear relationship between two variables 
$$x\sim\sim y$$

**Spearman's rho**: relationship between rankings of two variables 
$$x\sim\sim y$$

**Intraclass**: relationship between paired observations 
$$x_{t1}\sim\sim x_{t2}$$

---

## Calculating `r`

**Correlation**: the covariation divided by the total variation

$$r_{(x, y)} = \frac{cov_{(x, y)}}{s_{x}s_{y}}$$

+ $s_{x}$ and $s_{y}$ are the sample standard deviations of $x$ and $y$ 
   - *Note*: the square root of the variance, i.e., $\sqrt{s^2}$

---

## Notable (proof)

A single variable $x$ correlates perfectly with itself

$$r_{(x, x)} = \frac{cov_{(x, x)}}{s_{x}s_{x}}$$

+ We know from earlier $cov_{(x,x)}$ is equal to $s_{x}^2$ (variance)
+ And $s_{x}s_{x}$ can be rewritten to $s_{x}^2$, so...

$$r_{(x, x)} = \frac{s_{x}^2}{s_{x}^2} = 1.0$$


---

## Rank order correlation

Calculate **Spearman's rho** the same way, only convert the observations to ranked values first.

+ **Example**: converting observations 
+ `c(7, 3, 5, 1)` to `c(4, 2, 3, 1)`


---

## Interval/ratio data

```{r}
## create data frame with x and y coming from random normal draws
df <- data_frame(x = rnorm(20), y = x + rnorm(20))

## print
knitr::kable(df)
```



---

## Ordinal data

```{r, results='markup'}
## create data frame with rank (integer) data
df_ordered <- data_frame(
  x = sample(1:6, 20, replace = TRUE),
  y = round((x + sample(1:6, 20, replace = TRUE)) / 2, 0))

## print
p(df_ordered)
```


---

## `cor(method = "pearson")`

By default, `cor()` returns the Pearson product-moment correlation.

```{r}
## using cor defaults
cor(df$x, df$y)
```

Use `method` to specify the type of correlation and `use` to deal with missing data

```{r}
## pearson product-moment correlation
cor(df$x, df$y, method = "pearson", use = "complete.obs")
```

---

## `cor(method = ` "spearman"`)`

Pearson product moment correlation coefficient.

```{r}
## pearson product-moment correlation
cor(df_ordered$x, df_ordered$y, method = "pearson")
```

And Spearman's rho (rank) correlation coefficient.

```{r}
## spearman rho correlation
cor(df_ordered$x, df_ordered$y, method = "spearman")
```


---

## Hypothesis testing

To conduct a signficiance test of a correlation using R, use `cor.test()`. 

```{r}
## correlation test
cor.test(df$x, df$y)
```

.footnote[To convert the output to a more usable data frame, use `tidy()` from the **{broom}** package.]

---

## `method =` "pearson"

Correlation test of interval/ratio data
```{r}
## correlation sig test
cor.test(df$x, df$y, method = "pearson") %>%
  broom::tidy() %>% head()
```

---

## `method =` "spearman"
 
Correlation test of ordinal data
```{r}
## correlation sig test
cor.test(df$x, df$y, method = "spearman") %>%
  broom::tidy() %>% head()
```


---

## Correlation tool

Visualize correlations of different values (-1 to 1):

+ https://mikewk.shinyapps.io/correlation/

---

## Practice

Guess the correlation coefficient (the game):

+ http://guessthecorrelation.com/




---
class: inverse, center, middle

## Factors and factor analysis

---

## Factor

A **factor** is a psychometric term for variable. 

In statistics, factors describe the **latent** variables which we are attempting to measure. 


---

## Items

An **item** refers to a single question/prompt/response-provoking stimulus in a questionnaire.

In the context of a study, a **variable** refers to a construct (or factor) to be examined.

Variables can consist of one or more items from a questionnaire.


---

## Multi-item variables

When a variable (or factor) is measured using multiple items, we still want to represent it with one number.

**How can we represent a variable measured with 5 likert-type items using one number per observation (respondent)?**


---

## Example

Our variable of interest is extraversion/introversion. We measure it using four likert-type items:

+ I like talking to lots of people
+ I like attending events where I meet knew people
+ I am comfortable at a party where I don't know anyone else
+ Meeting my friends' friends makes me nervous


---

## Example responses

```{r}
## first person's responses
person1 <- c(1, 2, 1, 7)

## second person's responses
person2 <- c(6, 6, 5, 2)
```


---

## Factor analysis

To make sure we have **reliable** measures, we use factor analysis.

**Factor analysis** essentially finds the correlation between responses for similar items. 

There are lots of details and variations, but knowing this much will help you in the future!

